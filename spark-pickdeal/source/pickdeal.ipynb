{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import numpy as np \n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import base64\n",
    "from urllib.request import urlopen\n",
    "from bson import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_ITEMS = \"hdfs://worker-198:8020/user/fujinet/items/\"\n",
    "HDFS_IMAGE = \"hdfs://worker-198:8020/user/fujinet/images/\"\n",
    "HDFS_ARCHIVE = \"hdfs://worker-198:8020/user/fujinet/archive/device_data/\"\n",
    "CHECK_POINT = \"hdfs://worker-198:8020/user/fujinet/checkpoint\"\n",
    "SEAWEEDFS_IMAGE = 'http://worker-198:9888/images/'\n",
    "# SEAWEEDFS_IMAGE = 'http://192.168.137.57:9888/images/'\n",
    "JAR_WORKING = \"jars/*\"\n",
    "MONGODB_URI = \"mongodb://pickdeal:Abc12345@worker-198:27117\"# ?authSource=admin&retryWrites=true&w=majority\n",
    "DBNAME = \"pickdeal\"\n",
    "# CHECK_POINT = \"/home/fujinet/Desktop/huy-ndh/PickDeal/checkpoint\"\n",
    "KAFKA_SERVER = \"worker-198:9092\"\n",
    "KAFKA_TOPIC = \"items\"\n",
    "SPARK_LOGS = \"/home/fujinet/Documents/huy-ndh/PickDeal/spark.log\"\n",
    "AI_ENPOINTS = \"http://worker-198:6006/api/v1/pickdeal\"\n",
    "LOGO_CLASSIFICATION = f\"{AI_ENPOINTS }/logo-classification\"\n",
    "INFO_NER = f\"{AI_ENPOINTS }/ner-extraction\"\n",
    "SERVER_ENPOINTS = \"http://worker-198:4001/brands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    " \n",
    "logging.basicConfig(filename=SPARK_LOGS,\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logging.info(\"Running Spark\")\n",
    "\n",
    "logger = logging.getLogger('urbanGUI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/05 10:22:14 WARN Utils: Your hostname, worker-198 resolves to a loopback address: 127.0.1.1; using 172.16.1.198 instead (on interface enp6s0)\n",
      "23/10/05 10:22:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/05 10:22:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Process Files\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", True)\\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "    .config('spark.driver.extraClassPath', JAR_WORKING) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#     .config('spark.mongodb.write.convertJson', \"object_Or_Array_Only\") \\\n",
    "#     .config('spark.mongodb.write.convertJson', \"object_Or_Array_Only\") \\\n",
    "#     .config('spark.driver.extraClassPath', JAR_WORKING) \\\n",
    "#     .config('spark.jars.packages', 'org.mongodb:mongo-java-driver:3.10.2') \\\n",
    "#     .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.1.198:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming Process Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9105f64100>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# streaming_df = spark.readStream\\\n",
    "#     .format(\"json\") \\\n",
    "#     .option(\"cleanSource\", \"archive\") \\\n",
    "#     .option(\"sourceArchiveDir\", HDFS_ARCHIVE) \\\n",
    "#     .option(\"maxFilesPerTrigger\", 1) \\\n",
    "#     .load(HDFS_ITEMS)\n",
    "\n",
    "# schema_data = spark.read\\\n",
    "#     .format(\"json\") \\\n",
    "#     .load(HDFS_ITEMS)\n",
    "\n",
    "# schema = schema_data.schema\n",
    "schema = StructType([ \n",
    "    StructField(\"crawl_results\",ArrayType(StructType([\n",
    "        StructField(\"is_required\",BooleanType(),True),\n",
    "        StructField(\"key\",StringType(),True),\n",
    "        StructField(\"type\",StringType(),True),\n",
    "        StructField(\"value\",StringType(),True)\n",
    "    ])),True), \n",
    "    StructField(\"crawl_status_id\",StringType(),True), \n",
    "    StructField(\"host\",StringType(),True), \n",
    "    StructField(\"image_urls\", ArrayType(StringType()), True),\n",
    "    StructField(\"images\",ArrayType(StructType([\n",
    "        StructField(\"checksum\",StringType(),True),\n",
    "        StructField(\"path\",StringType(),True),\n",
    "        StructField(\"status\",StringType(),True),\n",
    "        StructField(\"url\",StringType(),True)\n",
    "    ])),True), \n",
    "    StructField(\"template_id\",StringType(),True), \n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"img_seaweed\", StringType(), True),\n",
    "    StructField(\"img_seaweed_path\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "\n",
    "streaming_df = spark.readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "      .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "      .option(\"startingOffsets\", \"latest\") \\\n",
    "      .load()\n",
    "\n",
    "streaming_df = streaming_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"), col(\"timestamp\").alias(\"timestamp_kafka\"))\n",
    "streaming_df = streaming_df.select(\"data.*\", \"timestamp_kafka\")\n",
    "\n",
    "# streaming_df.writeStream \\\n",
    "#             .format(\"console\")\\\n",
    "#             .outputMode(\"append\")\\\n",
    "#             .start()\\\n",
    "#             .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_ner = StructType([ \n",
    "    StructField(\"date\",ArrayType(StringType()),True),\n",
    "    StructField(\"brand\",StructType([\n",
    "        StructField(\"ner\",ArrayType(StringType()),True),\n",
    "        StructField(\"logo\",StructType([\n",
    "            StructField(\"content\",StringType(),True),\n",
    "            StructField(\"accuracy\",FloatType(),True)\n",
    "        ]),True)\n",
    "    ]),True), \n",
    "    StructField(\"location\",ArrayType(StringType()),True)\n",
    "  ])\n",
    "\n",
    "schema_id = StructType([ \n",
    "    StructField(\"oid\",StringType(),True),\n",
    "  ])\n",
    "\n",
    "def get_url_image (images):\n",
    "    images = r\"\"\"{}\"\"\".format(images)\n",
    "    imgs = json.loads(images)\n",
    "    \n",
    "    if len(imgs) > 0:\n",
    "        file_name = imgs[0]['path'].split('/')\n",
    "        return SEAWEEDFS_IMAGE + file_name[len(file_name) - 1]\n",
    "\n",
    "    return ''\n",
    "\n",
    "def get_description (crawl): \n",
    "    crawl = r\"\"\"{}\"\"\".format(crawl)\n",
    "    items = json.loads(crawl)\n",
    "    \n",
    "    for item in items:\n",
    "        if item['key'] == 'description':\n",
    "            return item['value']\n",
    "    return ''\n",
    "\n",
    "def get_title (crawl): \n",
    "    crawl = r\"\"\"{}\"\"\".format(crawl)\n",
    "    items = json.loads(crawl)\n",
    "    \n",
    "    for item in items:\n",
    "        if item['key'] == 'title':\n",
    "            return item['value']\n",
    "    return ''\n",
    "\n",
    "# def get_brand (host):\n",
    "#     domain = urlparse(host).netloc\n",
    "\n",
    "#     return domain\n",
    "\n",
    "def get_brand_image (url_img):\n",
    "    logo = { \n",
    "        \"content\": \"\", \n",
    "        \"accuracy\": 0\n",
    "    }\n",
    "    try:\n",
    "        img = urlopen(url_img)\n",
    "        data_to_upload=[]\n",
    "        data_to_upload.append((\"file\",  img))\n",
    "        res = requests.post(LOGO_CLASSIFICATION, files=data_to_upload)\n",
    "        \n",
    "        if (res.status_code == 200):\n",
    "            result = json.loads(res.text)[\"result\"]\n",
    "            brand = \"Unknow\"\n",
    "            max = 0\n",
    "            for e in result:\n",
    "                if e[\"prob\"] >= max:\n",
    "                    max = e[\"prob\"]\n",
    "                    brand = e[\"trade_mask\"]\n",
    "\n",
    "            if brand == \"Unknow\":\n",
    "                return logo\n",
    "            else:\n",
    "                brand = brand.replace(\"_\", \" \").title()\n",
    "                return { \"content\": brand, \"accuracy\": max}\n",
    "        else:\n",
    "            return logo\n",
    "    except:\n",
    "        return logo\n",
    "\n",
    "def get_info_description (url_img, text):\n",
    "    original_array = []\n",
    "    logo = get_brand_image(url_img)\n",
    "    ner = {    \n",
    "        \"date\": [],\n",
    "        \"brand\": {\n",
    "            \"logo\": logo,\n",
    "            \"ner\": []\n",
    "        },\n",
    "        \"location\": []\n",
    "    }   \n",
    "    try:\n",
    "        data = {\"text_data\": text}\n",
    "        res = requests.post(INFO_NER, data=data)\n",
    "        if (res.status_code == 200):\n",
    "            result = json.loads(json.loads(res.text)[\"result\"])\n",
    "            if isinstance(result, dict):\n",
    "                ner[\"brand\"][\"ner\"] = [i.title() for i in result[\"trademark\"]] \n",
    "            else:\n",
    "                if len(result) > 0:\n",
    "                    result_json = result[0]\n",
    "                    ner[\"date\"] = result_json[\"date\"]\n",
    "                    ner[\"brand\"][\"ner\"] = [i.title() for i in result_json[\"trademark\"]] \n",
    "                    ner[\"location\"] = result_json[\"location\"]\n",
    "        return json.dumps(ner)                           \n",
    "    except:\n",
    "        return json.dumps(ner)\n",
    "    \n",
    "def get_brand (info_extraction):\n",
    "    logo = info_extraction.brand.logo.content\n",
    "    ner = info_extraction.brand.ner\n",
    "    \n",
    "    if len(logo) > 0:\n",
    "        return logo\n",
    "    else:\n",
    "        if len(ner) > 0:\n",
    "            return ner[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "udf_struct_id = udf(\n",
    "    lambda x: tuple((str(x),)), \n",
    "    StructType([StructField(\"$oid\",  StringType(), True)])\n",
    ")\n",
    "\n",
    "def generate_object_id (id):\n",
    "    return O\n",
    "\n",
    "def add_brand (info_extraction):\n",
    "    brand = get_brand(info_extraction)\n",
    "    if brand is None:\n",
    "        brand= \"No brand\"\n",
    "        \n",
    "    try:\n",
    "        data = {\n",
    "            \"name\": brand, \n",
    "            \"description\": brand,\n",
    "        }\n",
    "\n",
    "        files = {\n",
    "            \"logo\": open('./no-image.png', 'rb')\n",
    "        }\n",
    "\n",
    "        res = requests.post(SERVER_ENPOINTS, data=data, files=files)\n",
    "        id_brand = json.loads(res.text)[\"id\"]\n",
    "        return id_brand\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch (udfdata, batchId):\n",
    "    udfdata.cache()\n",
    "    batch_df = udfdata\n",
    "    \n",
    "    print(f'{datetime.now()} \\t batchId:{batchId}; Row:{batch_df.count()}')\n",
    "    logging.info(f'batchId:{batchId}; Row:{batch_df.count()}')\n",
    "    \n",
    "    promotion_df = udfdata\n",
    "    item_df = udfdata\n",
    "    \n",
    "    promotion_df = promotion_df.withColumn(\"status\", lit(0))\n",
    "    promotion_df = promotion_df.withColumn(\"createdDate\", current_timestamp())\n",
    "    promotion_df = promotion_df.withColumn('imageList', to_json(promotion_df.images))\n",
    "#     promotion_df = promotion_df.withColumn(\"imageUrl\", udf(get_url_image)(promotion_df[\"imageList\"]))\n",
    "    promotion_df = promotion_df.withColumnRenamed(\"img_seaweed\", \"imageUrl\")\n",
    "    promotion_df = promotion_df.withColumnRenamed(\"img_seaweed_path\", \"imagePath\")\n",
    "#     promotion_df = promotion_df.withColumn(\"brand\", struct( \\\n",
    "#                                                                          udf(get_brand)(promotion_df[\"host\"]).alias(\"content\"), \\\n",
    "#                                                                          lit(random.randrange(50,100)/100).alias(\"accuracy\")))\n",
    "#     promotion_df = promotion_df.withColumn(\"type\", struct( \\\n",
    "#                                                                          lit(\"sale\").alias(\"content\"), \\\n",
    "#                                                                          lit(random.randrange(50,100)/100).alias(\"accuracy\")))\n",
    "#     promotion_df = promotion_df.withColumn(\"startDate\", struct( \\\n",
    "#                                                                          to_date(lit(start_date),'yyyy-MM-dd').alias(\"content\"), \\\n",
    "#                                                                          lit(random.randrange(50,100)/100).alias(\"accuracy\")))\n",
    "#     promotion_df = promotion_df.withColumn(\"expiryDate\", struct( \\\n",
    "#                                                                          to_date(lit(expiry_date),'yyyy-MM-dd').alias(\"content\"), \\\n",
    "#                                                                          lit(random.randrange(50,100)/100).alias(\"accuracy\")))\n",
    "    promotion_df = promotion_df.withColumnRenamed(\"timestamp\", \"timeStamp\")\n",
    "    promotion_df = promotion_df.withColumnRenamed(\"crawl_status_id\", \"crawlId\")\n",
    "    promotion_df = promotion_df.withColumn(\"crawlResults\", to_json(promotion_df.crawl_results))\n",
    "    promotion_df = promotion_df.withColumn(\"description\", udf(get_description)(promotion_df[\"crawlResults\"]))\n",
    "    promotion_df = promotion_df.withColumn(\"title\", udf(get_title)(promotion_df[\"crawlResults\"]))\n",
    "    promotion_df = promotion_df.withColumn(\"ner\", udf(get_info_description)(promotion_df[\"imageUrl\"], promotion_df[\"description\"]))\n",
    "    promotion_df = promotion_df.withColumn(\"extractInformation\", from_json(col(\"ner\"), schema_ner))\n",
    "    promotion_df = promotion_df.withColumn(\"brand\", udf(add_brand)(promotion_df[\"extractInformation\"]))\n",
    "#     promotion_df = promotion_df.withColumn(\"brand\", from_json(col(\"brandId\"), schema_id))\n",
    "    \n",
    "    promotion_df = promotion_df[['timeStamp', 'crawlId', 'createdDate', 'title', 'description', 'brand', 'imageUrl','imagePath', 'url', 'extractInformation', 'status']]\n",
    "    \n",
    "    item_df = item_df.withColumnRenamed(\"crawl_status_id\", \"crawlStatusId\")\n",
    "    item_df = item_df.withColumnRenamed(\"crawl_results\", \"crawlResults\")\n",
    "    item_df = item_df.withColumnRenamed(\"template_id\", \"templateId\")\n",
    "    item_df = item_df.withColumnRenamed(\"image_urls\", \"imageUrls\")\n",
    "\n",
    "    item_df = item_df[['crawlStatusId', 'timeStamp', 'host', 'url', 'templateId', 'imageUrls', 'images', 'crawlResults']]\n",
    "\n",
    "    promotion_df.write \\\n",
    "        .format(\"mongodb\") \\\n",
    "        .option(\"spark.mongodb.connection.uri\", MONGODB_URI) \\\n",
    "        .option(\"spark.mongodb.database\", DBNAME) \\\n",
    "        .option(\"spark.mongodb.collection\", \"promotions\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    \n",
    "    item_df.write \\\n",
    "        .format(\"mongodb\") \\\n",
    "        .option(\"spark.mongodb.connection.uri\", MONGODB_URI) \\\n",
    "        .option(\"spark.mongodb.database\", DBNAME) \\\n",
    "        .option(\"spark.mongodb.collection\", \"crawl_item\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    \n",
    "    udfdata.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/05 10:22:18 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-42b8fdee-587a-4a35-96f1-859326d5fb93. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/10/05 10:22:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f9105f8cd30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_df.writeStream \\\n",
    "    .foreachBatch(batch) \\\n",
    "    .start()\n",
    "\n",
    "#     .option(\"checkpointLocation\", CHECK_POINT) \\    .trigger(processingTime='60 seconds') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/05 10:22:19 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-05 10:22:20.757798 \t batchId:0; Row:0\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
